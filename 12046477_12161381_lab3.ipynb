{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Save this file as studentid1_studentid2_lab#.ipynb**\n",
    "(Your student-id is the number shown on your student card.)\n",
    "\n",
    "E.g. if you work with 3 people, the notebook should be named:\n",
    "12301230_3434343_1238938934_lab1.ipynb.\n",
    "\n",
    "**This will be parsed by a regexp, so please double check your filename.**\n",
    "\n",
    "Before you turn this problem in, please make sure everything runs correctly. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All). Note, that **you are not allowed to use Google Colab**.\n",
    "\n",
    "**Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your names and email adresses below.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Kai Liang\"\n",
    "NAME2 = \"Gongze Cao\"\n",
    "NAME3 = \"\"\n",
    "EMAIL = \"kai.liang@student.uva.nl\"\n",
    "EMAIL2 = \"asxaqz2@gmail.com\"\n",
    "EMAIL3 = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4087962852a82133f537bd51281c5c87",
     "grade": false,
     "grade_id": "cell-447a8ab4c82429ab",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Lab 3: Gaussian Processes and Support Vector Machines\n",
    "\n",
    "### Machine Learning 1, November 2018\n",
    "\n",
    "Notes on implementation:\n",
    "\n",
    "* You should write your code and answers in this IPython Notebook: http://ipython.org/notebook.html. If you have problems, please contact your teaching assistant.\n",
    "* Please write your answers right below the questions.\n",
    "* Among the first lines of your notebook should be \"%pylab inline\". This imports all required modules, and your plots will appear inline.\n",
    "* Refer to last week's lab notes, i.e. http://docs.scipy.org/doc/, if you are unsure about what function to use. There are different correct ways to implement each problem!\n",
    "* use the provided test boxes to check if your answers are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9c278ee9abc89a5ef5d829c1049141d2",
     "grade": false,
     "grade_id": "cell-a31fbe1e5a0de9bb",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "plt.rcParams[\"figure.figsize\"] = [20,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "510e268edf45424037e211654ddf6093",
     "grade": false,
     "grade_id": "cell-6d502beac900a992",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Install nb_conda with version 2.2.1 or newer, it is required for this assignment!\n"
     ]
    }
   ],
   "source": [
    "# This cell makes sure that you have all the necessary libraries installed\n",
    "\n",
    "import sys\n",
    "import platform\n",
    "from importlib.util import find_spec, module_from_spec\n",
    "\n",
    "def check_newer_version(version_inst, version_nec):\n",
    "    version_inst_split = version_inst.split('.')\n",
    "    version_nec_split = version_nec.split('.')\n",
    "    for i in range(min(len(version_inst_split), len(version_nec_split))):\n",
    "        if int(version_nec_split[i]) > int(version_inst_split[i]):\n",
    "            return False\n",
    "        elif int(version_nec_split[i]) < int(version_inst_split[i]):\n",
    "            return True\n",
    "    return True\n",
    "        \n",
    "    \n",
    "module_list = [('jupyter', '1.0.0'), \n",
    "               ('matplotlib', '2.0.2'), \n",
    "               ('numpy', '1.13.1'), \n",
    "               ('python', '3.6.2'), \n",
    "               ('sklearn', '0.19.0'), \n",
    "               ('scipy', '0.19.1'), \n",
    "               ('nb_conda', '2.2.1')]\n",
    "\n",
    "packages_correct = True\n",
    "packages_errors = []\n",
    "\n",
    "for module_name, version in module_list:\n",
    "    if module_name == 'scikit-learn':\n",
    "        module_name = 'sklearn'\n",
    "    if module_name == 'pyyaml':\n",
    "        module_name = 'yaml'\n",
    "    if 'python' in module_name:\n",
    "        python_version = platform.python_version()\n",
    "        if not check_newer_version(python_version, version):\n",
    "            packages_correct = False\n",
    "            error = f'Update {module_name} to version {version}. Current version is {python_version}.'\n",
    "            packages_errors.append(error) \n",
    "            print(error)\n",
    "    else:\n",
    "        spec = find_spec(module_name)\n",
    "        if spec is None:\n",
    "            packages_correct = False\n",
    "            error = f'Install {module_name} with version {version} or newer, it is required for this assignment!'\n",
    "            packages_errors.append(error) \n",
    "            print(error)\n",
    "        else:\n",
    "            x = __import__(module_name)\n",
    "            if hasattr(x, '__version__') and not check_newer_version(x.__version__, version):\n",
    "                packages_correct = False\n",
    "                error = f'Update {module_name} to version {version}. Current version is {x.__version__}.'\n",
    "                packages_errors.append(error) \n",
    "                print(error)\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    packages_correct = False\n",
    "    error = \"\"\"Please, don't use google colab!\n",
    "It will make it much more complicated for us to check your homework as it merges all the cells into one.\"\"\"\n",
    "    packages_errors.append(error) \n",
    "    print(error)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "packages_errors = '\\n'.join(packages_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7305e08b2c04f7ba3de1071d310b4a79",
     "grade": false,
     "grade_id": "cell-9f5845b06688e6e3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Part 1: Gaussian Processes\n",
    "\n",
    "For part 1 we will be refer to Bishop sections 6.4.2 and 6.4.3. You may also want to refer to Rasmussen's Gaussian Process text which is available online at http://www.gaussianprocess.org/gpml/chapters/ and especially to the project found at https://www.automaticstatistician.com/index/ by Ghahramani for some intuition in GP.  To understand Gaussian processes, it is highly recommended understand how marginal, partitioned Gaussian distributions can be converted into conditional Gaussian distributions.  This is covered in Bishop 2.3 and summarized in Eqns 2.94-2.98.\n",
    "$\\newcommand{\\bt}{\\mathbf{t}}$\n",
    "$\\newcommand{\\bx}{\\mathbf{x}}$\n",
    "$\\newcommand{\\by}{\\mathbf{y}}$\n",
    "$\\newcommand{\\bw}{\\mathbf{w}}$\n",
    "$\\newcommand{\\ba}{\\mathbf{a}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2c77bcc3c1b41adb15916666630bde13",
     "grade": false,
     "grade_id": "cell-c5f5f5b7b143efaa",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Periodic Data\n",
    "\n",
    "We will use the same data generating function that we used previously for regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8a43e884585a4210282ee5e5d42d047e",
     "grade": false,
     "grade_id": "cell-4f7ad28294ae4fe4",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def true_mean_function(x):\n",
    "    return np.sin(2*pi*(x+1))\n",
    "\n",
    "def add_noise(y, sigma):\n",
    "    return y + sigma*np.random.randn(len(y))\n",
    "\n",
    "def generate_t(x, sigma):\n",
    "    return add_noise(true_mean_function(x), sigma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d61908ff838f72d6bf5b7527be6f44fb",
     "grade": false,
     "grade_id": "cell-31ff6786c5cd8a8d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "sigma = 0.2\n",
    "beta  = 1.0 / pow(sigma, 2)\n",
    "N_test = 100\n",
    "\n",
    "x_test = np.linspace(-1, 1, N_test) \n",
    "mu_test = np.zeros(N_test)\n",
    "y_test = true_mean_function(x_test)\n",
    "t_test = add_noise(y_test, sigma)\n",
    "\n",
    "plt.plot( x_test, y_test, 'b-', lw=2)\n",
    "plt.plot( x_test, t_test, 'go')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "18312d35a0eb1f09149b49aae1657f9f",
     "grade": false,
     "grade_id": "cell-97e93e61042ddefb",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 1. Sampling from the Gaussian process prior (30 points)\n",
    "We will implement Gaussian process regression using the kernel function in Bishop Eqn. 6.63.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3fc49c57af98f449cc3628d5d0fd360e",
     "grade": false,
     "grade_id": "cell-853a18b4e2a92c05",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### 1.1 Why kernel function? (5 points)\n",
    "\n",
    "Before implementing kernel function, it would be useful to understand why kernels are crucial for high dimensional data. Why are kernels useful when you are dealing with high dimensional data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c1984cfc6b817e1797cd88b5ca6ee1da",
     "grade": true,
     "grade_id": "cell-057e3c5377f97420",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "89e06305ed3f154757062bbdf58032f9",
     "grade": false,
     "grade_id": "cell-919bc00e708dcbf1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### 1.2 k_n_m( xn, xm, thetas ) (5 points)\n",
    "To start, implement function `k_n_m(xn, xm, thetas)` that takes scalars $x_n$ and $x_m$, and a vector of $4$ thetas, and computes the kernel function Bishop Eqn. 6.63 (10 points).  NB: usually the kernel function will take $D$ by $1$ vectors, but since we are using a univariate problem, this makes things easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e9e7f20ba77a7ec760497ae005ddd509",
     "grade": false,
     "grade_id": "cell-10f11f2e5e6b38e6",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def k_n_m(xn, xm, thetas):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "bef2734ceed35587e9278c6c95da1208",
     "grade": true,
     "grade_id": "cell-8b621bd13ef5af5e",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT REMOVE THIS CELL!\n",
    "# It contains hidden tests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "afcc1b3cc15fa00330c698f845ec3cd4",
     "grade": false,
     "grade_id": "cell-ad0a764e2a661bc8",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### 1.3 computeK( X1, X2, thetas ) (10 points)\n",
    "Eqn 6.60 is the marginal distribution of mean output of $N$ data vectors: $p(\\mathbf{y}) = \\mathcal{N}(0, \\mathbf{K})$.  Notice that the expected mean function is $0$ at all locations, and that the covariance is a $N$ by $N$ kernel matrix $\\mathbf{K}$.  Write a function `computeK(x1, x2, thetas)`\n",
    "that computes the kernel matrix. Use k_n_m as part of an inner loop (of course, there are more efficient ways of computing the kernel function making better use of vectorization, but that is not necessary) (5 points).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "b211dee5ab3a8c03c0e231f7018a5f6c",
     "grade": false,
     "grade_id": "cell-a71c407c04df7096",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def computeK(x1, x2, thetas):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8794069f47c0efc03b065bda02d4cfa5",
     "grade": true,
     "grade_id": "cell-b306210055d7a91c",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### Test your function\n",
    "x1 = [0, 1, 2]\n",
    "x2 = [1, 2, 3, 4]\n",
    "thetas = [1, 2, 3, 4]\n",
    "K = computeK(x1, x2, thetas)\n",
    "\n",
    "\n",
    "assert K.shape == (len(x1), len(x2)), \"the shape of K is incorrect\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "da68357bed6c41606eaf48813b78358a",
     "grade": false,
     "grade_id": "cell-b57ae041bde18cd1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### 1.4 Plot function samples (15 points)\n",
    "Now sample mean functions at the x_test locations for the theta values in Bishop Figure 6.5, make a figure with a 2 by 3 subplot and make sure the title reflects the theta values (make sure everything is legible).  In other words, sample $\\by_i \\sim \\mathcal{N}(0, \\mathbf{K}_{\\theta})$.  Make use of numpy.random.multivariate_normal().  On your plots include the expected value of $\\by$ with a dashed line and fill_between 2 standard deviations of the uncertainty due to $\\mathbf{K}$ (the diagonal of $\\mathbf{K}$ is the variance of the model uncertainty) (15 points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "2d5e7a8803ed3dae7274a8d464ee8803",
     "grade": true,
     "grade_id": "cell-1424adaf2517b28b",
     "locked": false,
     "points": 15,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a5e1a0fa47bf4304d0d3e55d865a9fe5",
     "grade": false,
     "grade_id": "cell-2a25f52361101417",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2. Predictive distribution (35 points)\n",
    "So far we have sampled mean functions from the prior.  We can draw actual data $\\bt$ two ways.  The first way is generatively, by first sampling $\\by | \\mathbf{K}$, then sampling $\\bt | \\by, \\beta$ (Eqns 6.60 followed by 6.59).  The second way is to integrate over $\\by$ (the mean draw) and directly sample $\\bt | \\mathbf{K}, \\beta$ using Eqn 6.61.    This is the generative process for $\\bt$.  Note that we have not specified a distribution over inputs $\\bx$;  this is because Gaussian processes are conditional models.  Because of this we are free to generate locations $\\bx$ when playing around with the GP; obviously a dataset will give us input-output pairs.\n",
    "\n",
    "Once we have data, we are interested in the predictive distribution (note: the prior is the predictive distribution when there is no data).  Consider the joint distribution for $N+1$ targets, given by Eqn 6.64.  Its covariance matrix is composed of block components $C_N$, $\\mathbf{k}$, and $c$.  The covariance matrix $C_N$ for $\\bt_N$ is $C_N = \\mathbf{K}_N + \\beta^{-1}\\mathbf{I}_N$.  We have just made explicit the size $N$ of the matrix; $N$ is the number of training points.  The kernel vector $\\mathbf{k}$ is a $N$ by $1$ vector of kernel function evaluations between the training input data and the test input vector.  The scalar $c$ is a kernel evaluation at the test input.\n",
    "\n",
    "#### 2.1 gp_predictive_distribution(...) (10 points)\n",
    "Write a function `gp_predictive_distribution(x_train, t_train, x_test, theta, beta, C=None)` that computes  Eqns 6.66 and 6.67, except allow for an arbitrary number of test points (not just one) and now the kernel matrix is for training data.  By having C as an optional parameter, we can avoid computing it more than once (for this problem it is unimportant, but for real problems this is an issue).  The function should compute $\\mathbf{C}$, $\\mathbf{k}$, and return the mean, variance and $\\mathbf{C}$.  Do not forget: the computeK function computes $\\mathbf{K}$, not $\\mathbf{C}$.(10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "1379802e773fe565696ccb838b3093e6",
     "grade": false,
     "grade_id": "cell-eae0316765be4db6",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def gp_predictive_distribution(x_train, t_train, x_test, theta, beta, C=None):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return mean_test, covar_test, C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9f1c1578b671a1926f92c1f8c9d25b9e",
     "grade": true,
     "grade_id": "cell-9cc4442de9b765c1",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### Test your function\n",
    "N = 2\n",
    "train_x = np.linspace(-1, 1, N)\n",
    "train_t = 2*train_x\n",
    "test_N = 3\n",
    "test_x = np.linspace(-1, 1, test_N) \n",
    "theta = [1, 2, 3, 4]\n",
    "beta = 25\n",
    "test_mean, test_covar, C = gp_predictive_distribution(train_x, train_t, test_x, theta, beta, C=None)\n",
    "\n",
    "\n",
    "assert test_mean.shape == (test_N,), \"the shape of mean is incorrect\"\n",
    "assert test_covar.shape == (test_N, test_N), \"the shape of var is incorrect\"\n",
    "assert C.shape == (N, N), \"the shape of C is incorrect\"\n",
    "\n",
    "C_in = np.array([[0.804, -0.098168436], [-0.098168436, 0.804]])\n",
    "_, _, C_out = gp_predictive_distribution(train_x, train_t, test_x, theta, beta, C=C_in)\n",
    "\n",
    "assert np.allclose(C_in, C_out), \"C is not reused!\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "23db3fc8cc428c985f751486fd78b8be",
     "grade": false,
     "grade_id": "cell-32a51baa7ae3ee88",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### 2.2 gp_log_likelihood(...) (10 points)\n",
    "To learn the hyperparameters, we would need to compute the log-likelihood of the of the training data.  Implicitly, this is conditioned on the value setting for $\\mathbf{\\theta}$.  Write a function `gp_log_likelihood(x_train, t_train, theta, C=None, invC=None, beta=None)`, where C and invC can be stored and reused. It should return the log-likelihood, `C` and `invC`  (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "d5f359f9b03ed6c84b0e6a322d203152",
     "grade": false,
     "grade_id": "cell-b402394536823567",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def gp_log_likelihood(x_train, t_train, theta, beta, C=None, invC=None):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return lp, C, invC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "54892a8888dfda35d4898c70d7a2d14b",
     "grade": true,
     "grade_id": "cell-c21cca7e11e01d2f",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### Test your function\n",
    "N = 2\n",
    "train_x = np.linspace(-1, 1, N)\n",
    "train_t = 2 * train_x\n",
    "theta = [1, 2, 3, 4]\n",
    "beta = 25\n",
    "lp, C, invC = gp_log_likelihood(train_x, train_t, theta, beta, C=None, invC=None)\n",
    "\n",
    "assert lp < 0, \"the log-likelihood should be smaller than 0\"\n",
    "assert C.shape == (N, N), \"the shape of var is incorrect\"\n",
    "assert invC.shape == (N, N), \"the shape of C is incorrect\"\n",
    "\n",
    "C_in = np.array([[0.804, -0.098168436], [-0.098168436, 0.804]])\n",
    "_, C_out, _ = gp_log_likelihood(train_x, train_t, theta, beta, C=C_in, invC=None)\n",
    "\n",
    "assert np.allclose(C_in, C_out), \"C is not reused!\"\n",
    "\n",
    "invC_in = np.array([[1.26260453, 0.15416407], [0.15416407, 1.26260453]])\n",
    "_, _, invC_out = gp_log_likelihood(train_x, train_t, theta, beta, C=None, invC=invC_in)\n",
    "\n",
    "assert np.allclose(invC_in, invC_out), \"invC is not reused!\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a160e0d7511d31668ecef9642c17a86d",
     "grade": false,
     "grade_id": "cell-b8772e6321eac07f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### 2.3 Plotting (10 points)\n",
    "Repeat the 6 plots above, but this time conditioned on the training points.  Use the periodic data generator to create 2 training points where x is sampled uniformly between $-1$ and $1$.  For these plots, feel free to use the provided function \"gp_plot\".  Make sure you put the parameters in the title and this time also the log-likelihood. Try to understand the two types of uncertainty!  If you do not use `gp_plot(...)`, please add a fill between for the model and target noise. (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "38bf3cd51963f5ceec0fa0b1b4cc8b93",
     "grade": false,
     "grade_id": "cell-7bd5ca1b452daca8",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def gp_plot( x_test, y_test, mean_test, covar_test, x_train, t_train, theta, beta ):\n",
    "    # x_test: \n",
    "    # y_test:    the true function at x_test\n",
    "    # mean_test: predictive mean at x_test\n",
    "    # var_test:  predictive covariance at x_test \n",
    "    # t_train:   the training values\n",
    "    # theta:     the kernel parameters\n",
    "    # beta:      the precision (known)\n",
    "    \n",
    "    # the reason for the manipulation is to allow plots separating model and data stddevs.\n",
    "    std_total = np.sqrt(np.diag(covar_test))       # includes all uncertainty, model and target noise \n",
    "    std_model = np.sqrt(std_total**2 - 1.0/beta) # remove data noise to get model uncertainty in stddev\n",
    "    std_combo = std_model + np.sqrt(1.0/beta)    # add stddev (note: not the same as full)\n",
    "    \n",
    "    plt.plot(x_test, y_test, 'b', lw=3)\n",
    "    plt.plot(x_test, mean_test, 'k--', lw=2)\n",
    "    plt.fill_between(x_test, mean_test+2*std_combo,mean_test-2*std_combo, color='k', alpha=0.25)\n",
    "    plt.fill_between(x_test, mean_test+2*std_model,mean_test-2*std_model, color='r', alpha=0.25)\n",
    "    plt.plot(x_train, t_train, 'ro', ms=10)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "ae73d0e79c27fa0b71596e446f5acb52",
     "grade": true,
     "grade_id": "cell-1a3dbf1bd2a106f1",
     "locked": false,
     "points": 10,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "04e5a0ea661756676c10d6b8d7a11524",
     "grade": false,
     "grade_id": "cell-5709bf749ae02f84",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### 2.4 More plotting (5 points)\n",
    "Repeat the 6 plots above, but this time conditioned a new set of 10 training points. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "43da7f276e6ae7460306d00355c4b05d",
     "grade": true,
     "grade_id": "cell-b200d0aa0fb56cb7",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "12fb3774221d4ca2ac8cc75f45d2bec1",
     "grade": false,
     "grade_id": "cell-5d90eb9ba0ec6eed",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Part 2: Support Vector Machines (45 points)\n",
    "As seen in Part 1: Gaussian Processes, one of the significant limitations of many such algorithms is that the kernel function $k(\\bx_n , \\bx_m)$ must be evaluated for all possible pairs $\\bx_n$ and $\\bx_m$ of training points, which can be computationally infeasible during training and can lead to excessive computation times when making predictions for new data points.\n",
    "In Part 2: Support Vector Machines, we shall look at kernel-based algorithms that have sparse solutions, so that predictions for new inputs depend only on the kernel function evaluated at a subset of the training data points. We are using the same notation as in Bishop chapter 7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e0506a51ec128eeb17ace43d1a5c57d6",
     "grade": false,
     "grade_id": "cell-e89cb4e9ca837b57",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.1 Generating a linearly separable dataset (15 points)\n",
    "a) (5 points) First of all, we are going to create our own 2D toy dataset $X$. The dataset will consists of two i.i.d. subsets $X_1$ and $X_2$, each of the subsets will be sampled from a multivariate Gaussian distribution,\n",
    "\n",
    "\\begin{align}\n",
    "X_1 \\sim &\\mathcal{N}(\\mu_1, \\Sigma_1)\\\\\n",
    "&\\text{ and }\\\\\n",
    "X_2 \\sim &\\mathcal{N}(\\mu_2, \\Sigma_2).\n",
    "\\end{align}\n",
    "\n",
    "In the following, $X_1$ will have $N_1=20$ samples and a mean $\\mu_1=(1,1)$. $X_2$ will have $N_2=30$ samples and a mean $\\mu_2=(3,3)$.\n",
    "\n",
    "Plot the two subsets in one figure, choose two colors to indicate which sample belongs to which subset. In addition you should choose, $\\Sigma_1$ and $\\Sigma_2$ in a way that the two subsets become linearly separable. (Hint: Which form has the covariance matrix for a i.i.d. dataset?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "cc0901b713f288655ad60a2f1de76e59",
     "grade": true,
     "grade_id": "cell-497b9e4da2d7dd0d",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "def create_X1_and_X2():\n",
    "    n1 = 20\n",
    "    n2 = 30\n",
    "    mu1 = np.array([1,1])\n",
    "    mu2 = np.array([3,3])\n",
    "    sigma1 = np.diag(np.random.rand(2))\n",
    "    sigma2 = np.diag(np.random.rand(2))\n",
    "    X1 = random.multivariate_normal(mu1, sigma1, n1)\n",
    "    X2 = random.multivariate_normal(mu2, sigma2, n2)\n",
    "    return X1, X2\n",
    "X1, X2 = create_X1_and_X2()\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.scatter(X1[:,0],X1[:,1], label=\"X1\")\n",
    "plt.scatter(X2[:,0],X2[:,1], label=\"X2\")\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title(\"Visualisation of X1 and X2\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c9dc67da0bbba1c4fa2a5e292cd56a06",
     "grade": false,
     "grade_id": "cell-e82605073867be20",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "b) (10 points) In the next step we will combine the two datasets X_1, X_2 and generate a vector `t` containing the labels. Write a function `create_X_and_t(X1, X2)` it should return the combined data set X and the corresponding target vector t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "7fc0bfcc84c1a33bba7a1201e179192e",
     "grade": false,
     "grade_id": "cell-fb79685c3320a112",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def create_X_and_t(X1, X2):\n",
    "    # YOUR CODE HERE\n",
    "    X = np.concatenate((X1, X2))\n",
    "    t1 = np.ones(X1.shape[0])\n",
    "    t2 = -1 * np.ones(X2.shape[0])\n",
    "    t = np.concatenate((t1, t2))\n",
    "    return X, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f276a727ecde9a12abea3b14874b9424",
     "grade": true,
     "grade_id": "cell-0b007355061e9bf8",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### Test your function\n",
    "dim = 2\n",
    "N1_test = 3\n",
    "N2_test = 4\n",
    "X1_test = np.arange(6).reshape((N1_test, dim))\n",
    "X2_test = np.arange(8).reshape((N2_test, dim))\n",
    "X_test, t_test = create_X_and_t(X1_test, X2_test)\n",
    "\n",
    "\n",
    "assert X_test.shape == (N1_test + N2_test, dim), \"the shape of X is incorrect\"\n",
    "assert t_test.shape == (N1_test + N2_test,), \"the shape of t is incorrect\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0c99a9ce080ed437b70570895d15e2d4",
     "grade": false,
     "grade_id": "cell-9ba2051eb1a59b30",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.2 Finding the support vectors (15 points)\n",
    "Finally we going to use a SVM to obtain the decision boundary for which the margin is maximized. We have to solve the optimization problem\n",
    "\n",
    "\\begin{align}\n",
    "\\arg \\min_{\\bw, b} \\frac{1}{2} \\lVert \\bw \\rVert^2,\n",
    "\\end{align}\n",
    "\n",
    "subject to the constraints\n",
    "\n",
    "\\begin{align}\n",
    "t_n(\\bw^T \\phi(\\bx_n) + b) \\geq 1, n = 1,...,N.\n",
    "\\end{align}\n",
    "\n",
    "In order to solve this constrained optimization problem, we introduce Lagrange multipliers $a_n \\geq 0$. We obtain the dual\n",
    "representation of the maximum margin problem in which we maximize\n",
    "\n",
    "\\begin{align}\n",
    "\\sum_{n=1}^N a_n - \\frac{1}{2}\\sum_{n=1}^N\\sum_{m=1}^N a_n a_m t_n t_m k(\\bx_n, \\bx_m),\n",
    "\\end{align}\n",
    "\n",
    "with respect to a subject to the constraints\n",
    "\n",
    "\\begin{align}\n",
    "a_n &\\geq 0, n=1,...,N,\\\\\n",
    "\\sum_{n=1}^N a_n t_n &= 0.\n",
    "\\end{align}\n",
    "\n",
    "This takes the form of a quadratic programming problem in which we optimize a quadratic function of $\\mathbf{a}$, subject to a set of inequality constraints.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "07d5c4f152011be941d8c3de941643be",
     "grade": false,
     "grade_id": "cell-2737e7ded107f771",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "a) (5 points) In this example we will use a linear kernel $k(\\bx, \\bx') = \\bx^T\\bx'$. Write a function `computeK(X)` that computes the kernel matrix $K$ for the 2D dataset $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "6d85e675387e74c4b1f312572e42de4d",
     "grade": false,
     "grade_id": "cell-7d1a17d29190e696",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def computeK(X):\n",
    "    # YOUR CODE HERE\n",
    "    K = X @ X.T\n",
    "    return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f757e6dc60eac7f9499c72364f1ca521",
     "grade": true,
     "grade_id": "cell-da1dfa43730cf324",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "dim = 2\n",
    "N_test = 3\n",
    "X_test = np.arange(2, 8).reshape((N_test, dim))\n",
    "K_test = computeK(X_test)\n",
    "\n",
    "\n",
    "assert K_test.shape == (N_test, N_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "947bc481db9c0895aa9ba2b18dc8c12d",
     "grade": false,
     "grade_id": "cell-044564ecbbcaff3a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Next, we will rewrite the dual representation so that we can make use of computationally efficient vector-matrix multiplication. The objective becomes\n",
    "\n",
    "\\begin{align}\n",
    "\\min_{\\ba} \\frac{1}{2} \\ba^T K' \\ba - 1^T\\ba,\n",
    "\\end{align}\n",
    "\n",
    "subject to the constraints\n",
    "\n",
    "\\begin{align}\n",
    "a_n &\\geq 0, n=1,...,N,\\\\\n",
    "\\bt^T\\ba &= 0.\n",
    "\\end{align}\n",
    "\n",
    "Where\n",
    "\\begin{align}\n",
    "K'_{nm} = t_n t_m k(\\bx_n, \\bx_m),\n",
    "\\end{align}\n",
    "and in the special case of a linear kernel function,\n",
    "\\begin{align}\n",
    "K'_{nm} = t_n t_m k(\\bx_n, \\bx_m) = k(t_n \\bx_n, t_m \\bx_m).\n",
    "\\end{align}\n",
    "\n",
    "To solve the quadratic programming problem we will use a python module called cvxopt. You first have to install the module in your virtual environment (you have to activate it first), using the following command:\n",
    "\n",
    "`conda install -c anaconda cvxopt`\n",
    "\n",
    "The quadratic programming solver can be called as\n",
    "\n",
    "`cvxopt.solvers.qp(P, q[, G, h[, A, b[, solver[, initvals]]]])`\n",
    "\n",
    "This solves the following problem,\n",
    "\n",
    "\\begin{align}\n",
    "\\min_{\\bx} \\frac{1}{2} \\bx^T P \\bx + q^T\\bx,\n",
    "\\end{align}\n",
    "\n",
    "subject to the constraints,\n",
    "\n",
    "\\begin{align}\n",
    "G\\bx &\\leq h,\\\\\n",
    "A\\bx &= b.\n",
    "\\end{align}\n",
    "\n",
    "All we need to do is to map our formulation to the cvxopt interface.\n",
    "\n",
    "b) (10 points) Write a function `compute_multipliers(X, t)` that solves the quadratic programming problem using the cvxopt module and returns the lagrangian multiplier for every sample in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "a08a0f87f9dea85bc5fb0eaf47cb5824",
     "grade": false,
     "grade_id": "cell-5b4327394255f3a6",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named cvxopt",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-5ea6be9d3391>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcvxopt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompute_multipliers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# YOUR CODE HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named cvxopt"
     ]
    }
   ],
   "source": [
    "import cvxopt\n",
    "\n",
    "def compute_multipliers(X, t):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    P = cvxopt.matrix(K)\n",
    "    sol = cvxopt.solvers.qp(P, q, G, h, A, b)\n",
    "    a = np.array(sol['x'])\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "cc67648e58a44a9224e0eed7c093deaf",
     "grade": true,
     "grade_id": "cell-05dd3e69ab4290d5",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### Test your function\n",
    "dim = 2\n",
    "N_test = 3\n",
    "X_test = np.arange(2, 8).reshape((N_test, dim))\n",
    "t_test = np.array([-1., 1., 1.])\n",
    "a_test = compute_multipliers(X_test, t_test)\n",
    "\n",
    "\n",
    "assert a_test.shape == (N_test, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f39254febc169743b61bd19896fab2ba",
     "grade": false,
     "grade_id": "cell-79ee552a9c83325e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.3 Plot support vectors (5 points)\n",
    "Now that we have obtained the lagrangian multipliers $\\ba$, we use them to find our support vectors. Repeat the plot from 2.1, this time use a third color to indicate which samples are the support vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "b9281c423916582fe8b38c6494496099",
     "grade": true,
     "grade_id": "cell-313ecaa7ac15c36c",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d9da1b3e82eee6e95fdfd4b394a8fe7a",
     "grade": false,
     "grade_id": "cell-f2afbd01a7de87e8",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2.4 Plot the decision boundary (10 Points)\n",
    "The decision boundary is fully specified by a (usually very small) subset of training samples, the support vectors. Make use of\n",
    "\n",
    "\\begin{align}\n",
    "\\bw &= \\sum_{n=1}^N a_n t_n \\mathbf{\\phi}(\\bx_n)\\\\\n",
    "b &= \\frac{1}{N_S}\\sum_{n \\in S} (t_n - \\sum_{m \\in S} a_m t_m k(\\bx_n, \\bx_m)),\n",
    "\\end{align}\n",
    "\n",
    "where $S$ denotes the set of indices of the support vectors, to calculate the slope and intercept of the decision boundary. Generate a last plot that contains the two subsets, support vectors and decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "d4345049b6609f7e418b186b891d1e9f",
     "grade": true,
     "grade_id": "cell-f9511cd3c125aa65",
     "locked": false,
     "points": 10,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
